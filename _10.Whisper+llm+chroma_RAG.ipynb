{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463f25b8-ac50-4738-9735-8871ea94d599",
   "metadata": {},
   "source": [
    "# Chat with Audio Locally: A Guide to RAG with Whisper, Ollama, and Chromadb(can also use FAISS)\n",
    "Features\n",
    "1. Featured timestamp attached detection, for timestamp speech slice trace\n",
    "2. manual cosine similarity search for audio\n",
    "3. vector store similarity fetch docs for QA\n",
    "\n",
    "Inspired by: \n",
    "* https://medium.com/@ingridwickstevens/chat-with-your-audio-locally-a-guide-to-rag-with-whisper-ollama-and-faiss-6656b0b40a68\n",
    "* https://www.youtube.com/watch?v=TdMkKvzPe3E\n",
    "\n",
    "### 1. Transcribe audio to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065a090c-2027-4629-8700-864c918c87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from progress_bar_decorator import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991d84e7-a994-4f71-a306-e68c45b19b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mps', torch.float16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_mps = torch.backends.mps.is_available()\n",
    "has_cuda = torch.cuda.is_available()\n",
    "device = \"mps\" if has_mps else \"cuda\" if has_cuda else \"cpu\"\n",
    "torch_dtype = torch.float16 if has_mps else torch.float32\n",
    "device, torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60f8690-9390-4ae8-bed4-4ea3431cfb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"openai/whisper-large-v3\"\n",
    "# model_id = \"openai/whisper-medium\"\n",
    "\n",
    "hf_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, \n",
    "    use_safetensors=True,\n",
    "    cache_dir='/Users/leon/Documents/03.LLM/whisper/models/'\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67bcae50-ddcd-4dcb-9f73-3bfe2473f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=hf_model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,  # 128\n",
    "    chunk_length_s=64,   # 30 \n",
    "    batch_size=24,       # 16  \n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    ignore_warning=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b337125-6f4f-4f86-b4e1-99b79aafdeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.4 s, sys: 8.17 s, total: 41.5 s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "audio_file = './whisper/audio/如何通过Vision Pro理解ChatGPT.mp3'\n",
    "\n",
    "@progress_bar(expected_time=180)\n",
    "def transcribe():\n",
    "    result = pipe(\n",
    "        audio_file, \n",
    "        generate_kwargs={\"language\": \"Mandarin\",},\n",
    "        return_timestamps=True,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "result = transcribe()\n",
    "\n",
    "# clear memory\n",
    "del hf_model\n",
    "del processor\n",
    "del pipe\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5c197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如何通过Vision Pro理解ChatGPT\n"
     ]
    }
   ],
   "source": [
    "# parse file name\n",
    "file_name = audio_file.rpartition(\"/\")[-1].rpartition(\".\")[0]\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e237b25-c7cb-415a-8ac5-e35ca1a3c1b8",
   "metadata": {},
   "source": [
    "128, 64, 24\n",
    "CPU times: user 35.9 s, sys: 14.6 s, total: 50.4 s\n",
    "Wall time: 2min 53s\n",
    "++++++++++++++++++++++++\n",
    "128, 64, 20\n",
    "CPU times: user 59 s, sys: 12.9 s, total: 1min 11s\n",
    "Wall time: 2min 52s\n",
    "+++++++++++++++++++++++\n",
    "128, 60, 32\n",
    "CPU times: user 43.2 s, sys: 44.5 s, total: 1min 27s\n",
    "Wall time: 5min 7s\n",
    "++++++++++++++++++++++++\n",
    "128, 60, 16\n",
    "CPU times: user 1min, sys: 13.9 s, total: 1min 14s\n",
    "Wall time: 2min 57s\n",
    "+++++++++++++++++++++++++\n",
    "256, 60, 16\n",
    "CPU times: user 1min 35s, sys: 32.7 s, total: 2min 7s\n",
    "Wall time: 4min 58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da526316-29eb-4158-a1a4-ebdfe1a952a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'其实在GPT的大模型里面的知识是非常大的我们在现在只不过是activate它的其中一点点然后是通过这种instruct或者说是chat的方式去让这个模型的输出变得更palatable to human就是我们更容易理解它的思维模式可能远远在我们所能理解之上其实有一个问题就是你的prompt你要去让ChatGPT的五个问题我发现我其实写了一个slides然后才去年三月份今天我就会结合这个slides里的内容然后和我们这一年的观察给大家重温一下这五个问题然后带来一些新的见解像大家看到的呢我现在其实是在一个�可以去给大家去讲我的这个PPTLights upOh nice time第一个问题好像这样还是不太好玩我们还是回到我们原来的那个模式吧它这批地到底是什么它是一个范式突破吗我们在这个Vision Pro出来的时候在iPhone出来的时候在一个新科技出来的时候这都是一个我们最需要问的问题为什么这个问题重要因为它只有范式突破才能带来一个十倍百倍的机会如果你在原有的基础之上做一个就是小范围的突破而不是一个范式突破的话那它我们如何使用大圆模型第五个是我们人类和大圆模型有什么不一样这四个问题都是顺接的就是我们了解了它到底能产生什么样的影响尤其是站在这个技术刚刚出现的时候我们要去想象它五年后十年后的影响这样的话我们才可以知道它给我们带来的机会那最后呢其实是一个究极问题了就是我们站到这个技术已经发展到完全非常成熟的时候那人类和这个技术还有什么不一样就是说我们还应该去做什么才是不会被这个技术所改变的就是为什么这五个问题是这样的顺序和为什么这五个问题这么重要物理网上很多东西都是noise嘛再远一点吗再来一好像也就这么远了就是ChaiGBD它是一个叫做Generative Auto-Regressive Large Language Model它是一个代言模型然后这个代言模型它的本质是一个生成式的然后它是一个Auto-Regressive就是自回归式的那这个具体怎么理解就是它是这个是一个就是Stefan Wolfram的一个图生成下一个次然后你这样自然而然就可以把人类已知的文本和它去进行一个匹配看它生成的对不对这样的话你就有海量的标签好的数据去帮助你这个模型去学习之后就是这个他们非常重要的scaling law的这个observation我在这儿稍微再多解释就在这里就多解释一下吧为什么generative这个任务如此之重要标注都是所谓的labeled包括你的这个就是unsupervised learning其实你在一定程度上也必须要告诉这个模型的做得好还是不好那这样的数据其实是有限的和人需要去生成的也就是说你不能让模型去举一反三你只能让模型去做一个已经限定好的任务下去进行优化可是generative呢这个事情就变得非常的可以多学到这里面的知识和多给模型为高质量的文本接下来就是一个Leap of Faith就是你会相信这么一个大模型能产出好的结果这个Leap of Faith非常的重要但是OpenAI的人很明显是有这样的一个信仰的他们也做到了这些技术细节我们就不多聊了就是接下来一个技术很重要的点就是In-Context Learning然后这一点其实从一定程度上也是可以这么说的因为fantuning这个词并不是特别的精确所以呢我们这儿就发现fantuning其实是一个过度browt 过度宽泛的一个词那我们怎么样子去区别呢我们就区别过去的Machinery Model 是需要change the weight of the base model但是 in context learning 呢它其实是 activatethe different ways in the base model它这个是很重要的这就是区别了就traditional如果说一个high level的解释的话traditional ML是new task需要new modelin context learning呢可以让你做到new tasksame modeldifferent alignment然后第三个重要的点就是emergence就是涌现大模型的涌现是一个非常非常重要的点就是它不是线性的一点一点变好的而是过了一个节点突这个图像很可怕但是我觉得其实讲的还是有点道理的就是我们在背后呢是整个就是GPT的这个所谓的unsupervised learning当然这个词也不是完全准我觉得更准的是把这个unsupervised learning变成那个GPT就是他的base model然后supervised fine tuning我们把它叫做就是alignment然后reinforcement learning with human feedback这个可以是alignment中间的一环或者说是chat的方式去让这个模型的输出变得更palatable to human就是我们更容易理解或者说我们更容易appreciate其实它的思维模式可能远远在我们所能理解之上只不过我们没有办法理解模型在想什么我们需要用chat的方式去理解模型在想什么最后就是这个reinforcement learning with human feedback其实如果大家似乎是一样的方式其他的所有的点都是一样的只不过是它到底是用instruct去做align还是去用chat做align这就是未来其他很多模型就是OpenAI会出现很多模型的方法就是它不一定要用instruct或者chat它可以用别的方式去align模型然后去输出对应的效果来适配不同的任务种类最后一个那不是我们现在的这个做法我们要在左边那条路径上画个叉接下来我们是Funtune to activate different weights其实我们就把这一层其实应该改成叫alignment因为GPT有了in context learning的这样的一个涌现出来的能力导致我们可以通过alignment去activate不同的weight那在这个之下我们在用不同的方式我们接下来会说就是我们现在所整个GBT的Print Train这个Base Foundation Model其实是钢铁侠的那个反应核心就是它那个Arc它中间这个反应核心有了以后你才有可能做出钢铁侠其实上面那些武器人类都已经有了或者说Tony Stark就Stark Industry已经有了你没有中间的那个核心但是其实它的核心的质量比钢铁侠差很远其实就是钢铁侠2里边的那个鞭侠用鞭子的那个人你可以把它理解成一个开源模型它也能做出来一个核心然后它做的核心的效果也历史我就不多说了就是它到底是过去的发展Transformers的它到底是什么Transformers的这个发展我这之前都有兴趣的话其实应该去看俊林老师的文章就是我绝大多数的对这个知识是通过他的文章传起来的包括T5怎么样子把所有的task来成generative task这是一个非常重要的角色他在里边有说我也不多说了那接下来就进入我们的正式环节就是这五个问题第一个就是它是不是一个只是一个更好的大约模型这个其实是乐困的观点就是GPT有可能不一样的可能性过去的Motion Learning Model是什么就是我们要知道不一样的话我们就要知道过去的Motion Learning Model是什么过去的Motion Learning Model它其实就是Find Correspondence它是在数据里面寻找一些规律你可以告诉它你这个规律寻找的对不对最简单的就是你给它一起来你就可以让它做很多的事情这是过去的machine learning但是啊但是machine learning它有一个问题就是它只会英武学舍它不能理解当然这两个词都非常的重所以说我们接下来就要去讲这个什么是理解在这里边有一个window grade发现车是可以在红绿灯前面停下来的车可以压碎它的坚果然后它就会把坚果drop到这个红绿灯前面让车去把它压碎然后等到红灯的时候再去把这个坚果给pick up起来在这里边它只有一次任务就是它如果被车撞了就被撞死了所以说它所有过去的MOS learning你只能通过1000个乌鸦1万个乌鸦100万个乌鸦反复的去randomly尝试各种各样的pattern找到了以后然后再进化可是这样的代价就是必须要死这么多个乌鸦你是没有办法去在脑子里边simulate出来不同的效果最说的不对我来改这是一个非常强的推理能力这在GPT-3.5之前是没有一个模型哪怕GPT-2和3都没有展现出来的能力展现出来了这个以后我们觉得GPT确实和过去是不一样的它有了一个真正的理解的能力在那接下来就遇到了就是去年我在就是你在这样的一个auto-generativeauto-regressive generativelatch language models的范式之下能不能培养出来一个意识呢这就回到了一个哲学问题就是意识到底是什么我们也发现了一个观点就是人其实和猴子和其他的动物Biologically其实不是特别的不一样就是说我们就是你如果光看我们和猴子的大脑的区别你不会觉得你不会得出一个结论说人类就是和猴子完全不一样的东西那这样看起来我们的意识其实是emerge和以前是一个完全不一样的范式更新GPT到底是一个什么样的东西呢这个其实我在我的视频里面总结Gates和Altman的说法就是GUI加Morse law for everything第一个就是它是一个near perfect abstraction of internet technologies就是我们其实在过往的计算机解决了很多specific purpose的东西比如说我现在在看的这些东西都是跟GUI相关的我去刷抖音抖音是一个GUI去解决我的就是产生视频然后产生我的喜好把该我喜欢的视频推送到我面前然后高效率的去刷它都被GUI解决的很好但是拆JPT呢我要做一个什么那GPT就可以帮助你调用算力存储数据来去实现你的目的我有另外一个比喻就是GUI是山然后ChashGPT是水水涨船高迟早会淹没很多山的那我们接下来就是说就是intellectual就是我们智能而不只需要服务他面前的这个用户我希望在一两年之后我的GPT是可以做一个非常非常好的数据分析师或者数据科学家的这就是刚刚说的就是你做出来一个EraMan之后你给他再加一个差不多的弹脑那他其实自动的就可以去做很多很多的任务这个就是那个Outron对吧这个我就简单的跳过吧因为其实我已经说过很多遍了第四个就是我们应该怎么去使用它其实就是说你在浏览器刚出来的时候不要去再造一个浏览器而是应该去做网页这是回到刚才的那个文章我把最下面你在开放这一层就是你开放拆GPT可以去调用别的东西你可以写代码你可以去改变系统然后你在开放给一些指定的指令给拆GPT那它就能干很多事那再往上呢是开放你的这个alignment这个接口这我就不再多说了这个我们就直接看这一年的发布就好了相信都PM技巧就是你是要教给ChadGPT怎么用呢还是你是要告诉ChadGPT做什么我的想法是在一开始一定要有很强的工程能力去告诉ChadGPT怎么用然后在未来越来越重要的是告诉ChadGPT做什么我其实在这一年用ChadGPT做了很多尤其是我的数据分析我发现就是像我的一个随着我的调教变多的话如果说我已经把它非常好的调教了告诉他如何做了那我之后再告诉他你要去做什么就行了所以说总结一下的话就是一开始是有很多你要告诉这个大圆模型怎么做在未来更多的是告诉他做什么再说一些这个过往的历史吧就是XGBT这件事不是一个真的prediction他只不过把现有的东西放到了互联网然后就得到了巨大的财富但是在那之后最重要的就是十倍百倍的机会一定是新的科技做了过去科技所做不到的事情的就比如说GoogleGoogle在一开始的互联网上是没有用的因为那个时候互联网上都没有什么信息就是start with build simple things that people really wantInstagram其实之前就是一个filter app它不是一个social media或者怎么样现在的这些东西都是它做了一个人们真正需要的东西之后才出来的你没有人们需要的那个东西的话其他都是白谈你说那么多概念没有用的然后这就是我的一些opinion了就是它到底是一个2B2C的机会呢它是ScanIt吗我觉得不是但是按下private search就是retrieve这个环节非常重要的其实到现在ChaiGBT都没有做得很好它也是一个未来非常重要的方向那个Big Tech在这里边的会benefit什么其实是的因为他们已经有了现在有了场景他们其实可以把ChaiGBT在一开始用得很好最后一个问题就是人类和ChaiGBT的区别是什么就是ChaiGBT需要知道什么东西真正有用的然后去知道这个世界上缺的是什么有一词叫做Eureka就是阿基米德发明福利定律的那件或者发现福利定律的事情其实Eureka就是人类独特的能力然后这里边大家仔细去想一下Eureka的话我发现有两个步骤第一个就是你要能找到那个答案但是第二个是你要我在这1000个答案里边这个答案是真正重要的那个答案我们回到乔布斯其实乔布斯说的这些东西就是这些crazy ones他们就是知道这个答案并且能把这个答案通过自己的conviction然后去通过自己的行动去把它实现出来他们是真正change the world所以说你如果只是停留在一些表面的思大家感兴趣的话可以去看好的这里就是我们Vision Pro的一个看看刚才东西有没有录上希望大家喜欢或者说觉得这是一个有用的东西我的视频到那边去了那就先这样吧'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de65e382-b524-4436-a863-c0cf1f49cb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.48)</td>\n",
       "      <td>其实在GPT的大模型里面的知识是非常大的</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(4.48, 7.76)</td>\n",
       "      <td>我们在现在只不过是activate它的其中一点点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(7.76, 11.84)</td>\n",
       "      <td>然后是通过这种instruct或者说是chat的方式</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(11.84, 16.16)</td>\n",
       "      <td>去让这个模型的输出变得更palatable to human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(16.16, 17.68)</td>\n",
       "      <td>就是我们更容易理解</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>(1719.36, 1720.06)</td>\n",
       "      <td>一个</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>(1722.36, 1726.86)</td>\n",
       "      <td>看看刚才东西有没有录上</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>(1728.96, 1729.46)</td>\n",
       "      <td>希望大家喜欢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>(1731.76, 1733.36)</td>\n",
       "      <td>或者说觉得这是一个有用的东西我的视频到那边去了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>(1733.66, 1734.96)</td>\n",
       "      <td>那就先这样吧</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp                            text\n",
       "0           (0.0, 4.48)            其实在GPT的大模型里面的知识是非常大的\n",
       "1          (4.48, 7.76)        我们在现在只不过是activate它的其中一点点\n",
       "2         (7.76, 11.84)      然后是通过这种instruct或者说是chat的方式\n",
       "3        (11.84, 16.16)  去让这个模型的输出变得更palatable to human\n",
       "4        (16.16, 17.68)                       就是我们更容易理解\n",
       "..                  ...                             ...\n",
       "217  (1719.36, 1720.06)                              一个\n",
       "218  (1722.36, 1726.86)                     看看刚才东西有没有录上\n",
       "219  (1728.96, 1729.46)                          希望大家喜欢\n",
       "220  (1731.76, 1733.36)         或者说觉得这是一个有用的东西我的视频到那边去了\n",
       "221  (1733.66, 1734.96)                          那就先这样吧\n",
       "\n",
       "[222 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_transcribe = pd.DataFrame(result['chunks'])\n",
    "df_transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2bee58-55f1-4288-8480-9b543c4aa3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse timestamp function\n",
    "def parse_audio_slice_timestamp(time_tuple):\n",
    "    time_list = list(time_tuple)\n",
    "    return time_list[0], time_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19dd35b9-f0d2-482e-887a-33255f5873b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.48)</td>\n",
       "      <td>其实在GPT的大模型里面的知识是非常大的</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(4.48, 7.76)</td>\n",
       "      <td>我们在现在只不过是activate它的其中一点点</td>\n",
       "      <td>4.48</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(7.76, 11.84)</td>\n",
       "      <td>然后是通过这种instruct或者说是chat的方式</td>\n",
       "      <td>7.76</td>\n",
       "      <td>11.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(11.84, 16.16)</td>\n",
       "      <td>去让这个模型的输出变得更palatable to human</td>\n",
       "      <td>11.84</td>\n",
       "      <td>16.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(16.16, 17.68)</td>\n",
       "      <td>就是我们更容易理解</td>\n",
       "      <td>16.16</td>\n",
       "      <td>17.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                            text  start    end\n",
       "0     (0.0, 4.48)            其实在GPT的大模型里面的知识是非常大的   0.00   4.48\n",
       "1    (4.48, 7.76)        我们在现在只不过是activate它的其中一点点   4.48   7.76\n",
       "2   (7.76, 11.84)      然后是通过这种instruct或者说是chat的方式   7.76  11.84\n",
       "3  (11.84, 16.16)  去让这个模型的输出变得更palatable to human  11.84  16.16\n",
       "4  (16.16, 17.68)                       就是我们更容易理解  16.16  17.68"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe_filename = f'./whisper/transcribe/{file_name}.csv'\n",
    "\n",
    "df_transcribe.loc[:, 'start'] = df_transcribe['timestamp'].apply(lambda x: list(x)[0])\n",
    "df_transcribe.loc[:, 'end'] = df_transcribe['timestamp'].apply(lambda x: list(x)[1])\n",
    "df_transcribe.to_csv(transcribe_filename, index=False)\n",
    "df_transcribe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0aa8964-bf8a-4f2c-8048-7ec926bf1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_text_filename = f'./whisper/transcribe/{file_name}.txt'\n",
    "\n",
    "with open(transcribe_text_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b98f90-7864-4847-aad6-61aaef645286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of this audio file 28.99 minutes\n",
      "Text: 它是在数据里面寻找一些规律\n",
      "Playing audio slice start from 14.4185m to 14.4695m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmp6tp9c1_r.wav':\n",
      "  Duration: 00:00:03.06, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   2.85 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   2.97 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "sound = AudioSegment.from_file(audio_file)\n",
    "print(f'Length of this audio file {round(len(sound)/1000/60, 2)} minutes')\n",
    "\n",
    "row = df_transcribe.iloc[int(len(df_transcribe)/2), :]\n",
    "print('Text:', row['text'])\n",
    "print('Playing audio slice start from {}m to {}m'.format(row['start']/60, row['end']/60))\n",
    "\n",
    "# audio timestamp in ms, hence times 1000\n",
    "play(sound[row['start']*1000: row['end']*1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3de934c-6d6d-4cc6-9181-a76eceac8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play(sound[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa7dad3-b2af-4461-891e-306398e36c73",
   "metadata": {},
   "source": [
    "### 2. Tokenize and embed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d542652-17a4-45d5-8866-79af272f24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef560c2c-8c40-47c9-9cb9-e4d2bc521bfd",
   "metadata": {},
   "source": [
    "#### 2.1 Direct embedding against audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c517c7f-6a5d-4869-9adb-68354c5bc60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.48)</td>\n",
       "      <td>其实在GPT的大模型里面的知识是非常大的</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(4.48, 7.76)</td>\n",
       "      <td>我们在现在只不过是activate它的其中一点点</td>\n",
       "      <td>4.48</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(7.76, 11.84)</td>\n",
       "      <td>然后是通过这种instruct或者说是chat的方式</td>\n",
       "      <td>7.76</td>\n",
       "      <td>11.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(11.84, 16.16)</td>\n",
       "      <td>去让这个模型的输出变得更palatable to human</td>\n",
       "      <td>11.84</td>\n",
       "      <td>16.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(16.16, 17.68)</td>\n",
       "      <td>就是我们更容易理解</td>\n",
       "      <td>16.16</td>\n",
       "      <td>17.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                            text  start    end\n",
       "0     (0.0, 4.48)            其实在GPT的大模型里面的知识是非常大的   0.00   4.48\n",
       "1    (4.48, 7.76)        我们在现在只不过是activate它的其中一点点   4.48   7.76\n",
       "2   (7.76, 11.84)      然后是通过这种instruct或者说是chat的方式   7.76  11.84\n",
       "3  (11.84, 16.16)  去让这个模型的输出变得更palatable to human  11.84  16.16\n",
       "4  (16.16, 17.68)                       就是我们更容易理解  16.16  17.68"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transcribe_filename = f'./whisper/transcribe/{file_name}.csv'\n",
    "\n",
    "df_embed = pd.read_csv(transcribe_filename)\n",
    "df_embed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db490dc",
   "metadata": {},
   "source": [
    "##### Choose embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11076c81-2a03-436b-8949-bae35993900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings, SentenceTransformerEmbeddings\n",
    "# embeddings = OllamaEmbeddings(model='llama2-chinese:latest')\n",
    "# embeddings = OllamaEmbeddings(model='mxbai-embed-large:latest')\n",
    "# embeddings = OllamaEmbeddings(model='nomic-embed-text:latest')\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name='BAAI/bge-large-zh-v1.5', \n",
    "    cache_folder='/Users/leon/Documents/03.LLM/embedding_models'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0827d497-b942-405d-82db-5a9acd11c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda function to embed audio text\n",
    "add_embed = lambda x: embeddings.embed_query(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b81b22-7fa4-4e9a-94cb-874089764d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similiarity search function\n",
    "import numpy as np\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94363d13-969c-4870-a87c-707bc7cec4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.87 s, sys: 734 ms, total: 6.6 s\n",
      "Wall time: 15.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 4.48)</td>\n",
       "      <td>其实在GPT的大模型里面的知识是非常大的</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.48</td>\n",
       "      <td>[0.020624252036213875, 0.033191632479429245, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(4.48, 7.76)</td>\n",
       "      <td>我们在现在只不过是activate它的其中一点点</td>\n",
       "      <td>4.48</td>\n",
       "      <td>7.76</td>\n",
       "      <td>[0.026757562533020973, 0.008384518325328827, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(7.76, 11.84)</td>\n",
       "      <td>然后是通过这种instruct或者说是chat的方式</td>\n",
       "      <td>7.76</td>\n",
       "      <td>11.84</td>\n",
       "      <td>[0.05034959688782692, 0.0021004120353609324, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(11.84, 16.16)</td>\n",
       "      <td>去让这个模型的输出变得更palatable to human</td>\n",
       "      <td>11.84</td>\n",
       "      <td>16.16</td>\n",
       "      <td>[0.022767867892980576, -0.001781404484063387, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(16.16, 17.68)</td>\n",
       "      <td>就是我们更容易理解</td>\n",
       "      <td>16.16</td>\n",
       "      <td>17.68</td>\n",
       "      <td>[0.014743323437869549, -0.043621234595775604, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                            text  start    end  \\\n",
       "0     (0.0, 4.48)            其实在GPT的大模型里面的知识是非常大的   0.00   4.48   \n",
       "1    (4.48, 7.76)        我们在现在只不过是activate它的其中一点点   4.48   7.76   \n",
       "2   (7.76, 11.84)      然后是通过这种instruct或者说是chat的方式   7.76  11.84   \n",
       "3  (11.84, 16.16)  去让这个模型的输出变得更palatable to human  11.84  16.16   \n",
       "4  (16.16, 17.68)                       就是我们更容易理解  16.16  17.68   \n",
       "\n",
       "                                          text_embed  \n",
       "0  [0.020624252036213875, 0.033191632479429245, -...  \n",
       "1  [0.026757562533020973, 0.008384518325328827, -...  \n",
       "2  [0.05034959688782692, 0.0021004120353609324, 0...  \n",
       "3  [0.022767867892980576, -0.001781404484063387, ...  \n",
       "4  [0.014743323437869549, -0.043621234595775604, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_embed.loc[:, 'text_embed'] = df_embed.apply(add_embed, axis=1)\n",
    "df_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b2b6d35-f1d0-4bed-ab19-1a799e58a5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check embeded vector length\n",
    "len(df_embed['text_embed'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64667d07-b2af-4657-a3b4-dd7c99067f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give your search query\n",
    "search_term = 'In-Context Learning'\n",
    "search_term_embed = embeddings.embed_query(search_term)\n",
    "# len(search_term_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "206c330a-24ee-401a-8c1c-f47a39a7443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>text_embed</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>(444.11, 445.87)</td>\n",
       "      <td>但是 in context learning 呢</td>\n",
       "      <td>444.11</td>\n",
       "      <td>445.87</td>\n",
       "      <td>[0.044630419462919235, -0.01232170220464468, 0...</td>\n",
       "      <td>0.794197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(651.6, 654.84)</td>\n",
       "      <td>因为GPT有了in context learning的这样的一个</td>\n",
       "      <td>651.60</td>\n",
       "      <td>654.84</td>\n",
       "      <td>[0.02766631357371807, -0.006514569744467735, -...</td>\n",
       "      <td>0.629797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>(401.86, 437.63)</td>\n",
       "      <td>就是接下来一个技术很重要的点就是In-Context Learning然后这一点其实从一定程...</td>\n",
       "      <td>401.86</td>\n",
       "      <td>437.63</td>\n",
       "      <td>[0.024835968390107155, 0.012752030044794083, 0...</td>\n",
       "      <td>0.587457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>(533.0, 535.8)</td>\n",
       "      <td>然后reinforcement learning with human feedback</td>\n",
       "      <td>533.00</td>\n",
       "      <td>535.80</td>\n",
       "      <td>[0.02373799867928028, 0.01962607353925705, 0.0...</td>\n",
       "      <td>0.561551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>(576.19, 608.05)</td>\n",
       "      <td>reinforcement learning with human feedback其实如果...</td>\n",
       "      <td>576.19</td>\n",
       "      <td>608.05</td>\n",
       "      <td>[0.04316745698451996, 0.01569114439189434, 0.0...</td>\n",
       "      <td>0.531998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp                                               text  \\\n",
       "59  (444.11, 445.87)                           但是 in context learning 呢   \n",
       "87   (651.6, 654.84)                   因为GPT有了in context learning的这样的一个   \n",
       "54  (401.86, 437.63)  就是接下来一个技术很重要的点就是In-Context Learning然后这一点其实从一定程...   \n",
       "73    (533.0, 535.8)       然后reinforcement learning with human feedback   \n",
       "81  (576.19, 608.05)  reinforcement learning with human feedback其实如果...   \n",
       "\n",
       "     start     end                                         text_embed  \\\n",
       "59  444.11  445.87  [0.044630419462919235, -0.01232170220464468, 0...   \n",
       "87  651.60  654.84  [0.02766631357371807, -0.006514569744467735, -...   \n",
       "54  401.86  437.63  [0.024835968390107155, 0.012752030044794083, 0...   \n",
       "73  533.00  535.80  [0.02373799867928028, 0.01962607353925705, 0.0...   \n",
       "81  576.19  608.05  [0.04316745698451996, 0.01569114439189434, 0.0...   \n",
       "\n",
       "    cosine_similarity  \n",
       "59           0.794197  \n",
       "87           0.629797  \n",
       "54           0.587457  \n",
       "73           0.561551  \n",
       "81           0.531998  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conduct similiarity and sorting\n",
    "df_embed.loc[:, 'cosine_similarity'] = df_embed['text_embed'].apply(lambda x: cosine_similarity(x, search_term_embed))\n",
    "df_sorted = df_embed.sort_values(by='cosine_similarity', ascending=False)\n",
    "df_sorted.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce8a8007-a25d-4fd4-b56a-2f9201f06fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpki_o2mf4.wav':\n",
      "  Duration: 00:00:01.60, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.51 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpkryd8zs5.wav':\n",
      "  Duration: 00:00:01.04, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   0.96 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmp680ma925.wav':\n",
      "  Duration: 00:00:02.32, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   2.26 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpxkwck7w0.wav':\n",
      "  Duration: 00:00:38.89, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "  38.80 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmp6491kuwz.wav':\n",
      "  Duration: 00:00:02.94, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   2.83 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   2.87 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "# playsound for top 5 ranking\n",
    "for index, row in df_sorted.iloc[:5].iterrows():\n",
    "    play(sound[row.start*1000: row.end*1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "391cd43a-e30c-450e-a4d9-384b3ca02870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# playback function \n",
    "def playback_by_query(query, k=3, show_text=False):\n",
    "    \"\"\"\n",
    "    A quick playback function\n",
    "    \n",
    "    Parameters:\n",
    "    query: str, a user query for similarity search\n",
    "    k: int, number of top results to search\n",
    "    \"\"\"\n",
    "    search_term = query\n",
    "    search_term_embed = embeddings.embed_query(search_term)\n",
    "\n",
    "    # conduct similiarity and sorting\n",
    "    df_embed.loc[:, 'cosine_similarity'] = df_embed['text_embed'].apply(lambda x: cosine_similarity(x, search_term_embed))\n",
    "    df_sorted = df_embed.sort_values(by='cosine_similarity', ascending=False)\n",
    "\n",
    "    if show_text:\n",
    "        display(df_sorted.iloc[:k]['text'])\n",
    "\n",
    "    for index, row in df_sorted.iloc[:k].iterrows():\n",
    "        play(sound[row.start*1000: row.end*1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aae0bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpmicqjme_.wav':\n",
      "  Duration: 00:00:01.16, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.07 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpo08r5v6g.wav':\n",
      "  Duration: 00:00:01.80, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.74 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmptycdb4g5.wav':\n",
      "  Duration: 00:00:01.84, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.72 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   1.75 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    }
   ],
   "source": [
    "playback_by_query(\"model inference\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a446f-82e8-4cdd-b236-2ce12c36a404",
   "metadata": {},
   "source": [
    "#### 2.2 Embedding for LLM-based RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a560762d-6c9c-455a-8f92-3fcd24bb1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text to split\n",
    "# transcribe_text_filename = f'./whisper/transcribe/{file_name}.txt'\n",
    "with open(transcribe_text_filename, 'r') as f:\n",
    "    transcribe_text = f.read()\n",
    "\n",
    "# split the text into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = splitter.split_text(transcribe_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ca2558e-198e-4bc3-b408-65cfa395f8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,\n",
       " 'with build simple things that people really wantInstagram其实之前就是一个filter app它不是一个social media或者怎么样现在的这些东西都是它做了一个人们真正需要的东西之后才出来的你没有人们需要的那个东西的话其他都是白谈你说那么多概念没有用的然后这就是我的一些opinion了就是它到底是一个2B2C的机会呢它是ScanIt吗我觉得不是但是按下private search就是retrieve这个环节非常重要的其实到现在ChaiGBT都没有做得很好它也是一个未来非常重要的方向那个Big')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), texts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fba2f7fc-2465-42d9-bdc5-6aad85047dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store using Chroma, in-memory without setting persistant cache\n",
    "speech_vector = Chroma.from_texts(\n",
    "    texts, \n",
    "    embedding=embeddings, \n",
    "    metadatas=[{'source': str(i)} for i in range(len(texts))],\n",
    "    collection_name='speech-rag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5d603-3ff0-4e6e-85a9-f58c2fb3c85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd20b5a-345a-4118-b5af-cab4ce9373a5",
   "metadata": {},
   "source": [
    "### 3.Setup LLM and Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff447bc8-bc53-4643-96d2-3e0dadfe9b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                          \tID          \tSIZE  \tMODIFIED     \n",
      "brxce/stable-diffusion-prompt-generator:latest\t474a09318a2e\t4.1 GB\t6 days ago  \t\n",
      "codellama:7b-python-fp16                      \tc586d7593fc9\t13 GB \t7 days ago  \t\n",
      "codellama:latest                              \t8fdf8f752f6e\t3.8 GB\t7 days ago  \t\n",
      "command-r:35b-v0.1-q6_K                       \tc46e949ec735\t28 GB \t2 weeks ago \t\n",
      "dolphin-llama3:latest                         \t613f068e29f8\t4.7 GB\t7 days ago  \t\n",
      "llama2:13b-f16                                \t18051f2e82e3\t26 GB \t4 weeks ago \t\n",
      "llama2:7b-f32                                 \t4901050728fc\t26 GB \t4 weeks ago \t\n",
      "llama2-chinese:13b-chat-fp16                  \t3d4c5a00962c\t26 GB \t4 weeks ago \t\n",
      "llama2-chinese:7b-chat-fp16                   \tb73150f2949c\t13 GB \t4 weeks ago \t\n",
      "llama3:70b-instruct-q4_0                      \tbcfb190ca3a7\t39 GB \t13 days ago \t\n",
      "llama3:8b-instruct-fp16                       \tc1d0ea97005c\t16 GB \t13 days ago \t\n",
      "llava:34b-v1.6-q6_K                           \t8f572ea02185\t28 GB \t2 weeks ago \t\n",
      "mistral:7b-instruct-v0.2-fp16                 \t094d67ff087c\t14 GB \t2 weeks ago \t\n",
      "mixtral:latest                                \t7708c059a8bb\t26 GB \t4 weeks ago \t\n",
      "mxbai-embed-large:latest                      \t468836162de7\t669 MB\t3 weeks ago \t\n",
      "nomic-embed-text:latest                       \t0a109f422b47\t274 MB\t3 weeks ago \t\n",
      "qwen:14b                                      \t80362ced6553\t8.2 GB\t8 days ago  \t\n",
      "starcoder2:15b                                \t20cdb0f709c2\t9.1 GB\t41 hours ago\t\n",
      "starcoder2:3b-fp16                            \t118317b2c129\t6.1 GB\t41 hours ago\t\n",
      "wizardlm2:7b-fp16                             \ta34a3bbd552b\t14 GB \t2 weeks ago \t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aacd20b-157f-4441-88b8-b76e5ce6b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "# setup llm\n",
    "# local_llm = 'llama3:8b-instruct-fp16'\n",
    "# local_llm = 'command-r:35b-v0.1-q6_K'\n",
    "# local_llm = 'wizardlm2:7b-fp16'\n",
    "# local_llm = 'mistral:7b-instruct-v0.2-fp16'\n",
    "local_llm = 'qwen:14b'\n",
    "\n",
    "llm = Ollama(model=local_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95feb79c-5fdf-4658-abad-dd07fd1c43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms.chatglm3 import ChatGLM3\n",
    "\n",
    "# llm = ChatGLM3(\n",
    "#     model='chatglm3-6b',\n",
    "#     endpoint_url='http://127.0.0.1:8000/v1/chat/completions',\n",
    "#     verbose=True\n",
    "# )\n",
    "# llm.invoke('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ea6ab88-cbaa-454d-be8e-baa1d2d38372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a1bdacf-66dd-4d6b-a4b7-aaf62697f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RAG prompt\n",
    "rag_prompt = ChatPromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=['context', 'question'],\n",
    "                # template=\"\"\"You answer questions about the contents of a transcribed audio file.\n",
    "                # Use only the provided audio file transcription as context to answer the question. \n",
    "                # Do not use any additional information.\n",
    "                # If you don't know the answer, just say that you don't know. Do not use external knowledge. \n",
    "                # Use three sentences maximum and keep the answer concise. \n",
    "                # Make sure to reference your sources with quotes of the provided context as citations.\n",
    "                # \\nQuestion: {question} \\nContext: {context} \\nAnswer:\n",
    "                # \"\"\",\n",
    "                template=\"\"\"你针对会议录音转的文字内容回答问题。\n",
    "                只利用录音转的文字内容作为上下文来回答问题。\n",
    "                不要使用任何其它额外信息。\n",
    "                如果你不知道答案，就回答不知道，不要使用外部知识。\n",
    "                用最多五句话来回答，并确保答案准确。\n",
    "                确保在答案中对上下文的源信息进行引用。\n",
    "                \\nQuestion: {question} \\nContext: {context} \\nAnswer:\n",
    "                \"\"\"\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77e4d90a-dfff-4cc5-ab7b-5406e59b58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load qa chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "chain = load_qa_chain(llm=llm, chain_type='stuff', prompt=rag_prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87d18d-5122-48e3-995e-28ce97c09e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c24b27a-76b7-4c78-a576-831bb3b8e75a",
   "metadata": {},
   "source": [
    "### Query and Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40a46c27-f80e-4a22-a0e4-8481eb842b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a query\n",
    "query = '如何提升大模型的推理能力？'\n",
    "# query = '监管政策的解读'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "678f1852-c9fa-43c8-bd20-98494f4ae3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='learning呢可以让你做到new tasksame modeldifferent alignment然后第三个重要的点就是emergence就是涌现大模型的涌现是一个非常非常重要的点就是它不是线性的一点一点变好的而是过了一个节点突这个图像很可怕但是我觉得其实讲的还是有点道理的就是我们在背后呢是整个就是GPT的这个所谓的unsupervised learning当然这个词也不是完全准我觉得更准的是把这个unsupervised learning变成那个GPT就是他的base model然后supervised fine tuning我们把它叫做就是alignment然后reinforcement learning with human feedback这个可以是alignment中间的一环或者说是chat的方式去让这个模型的输出变得更palatable to human就是我们更容易理解或者说我们更容易appreciate其实它的思维模式可能远远在我们所能理解之上只不过我们没有办法理解模型在想什么我们需要用chat的方式去理解模型在想什么最后就是这个reinforcement', metadata={'source': '4'}),\n",
       " Document(page_content='Model是什么过去的Motion Learning Model它其实就是Find Correspondence它是在数据里面寻找一些规律你可以告诉它你这个规律寻找的对不对最简单的就是你给它一起来你就可以让它做很多的事情这是过去的machine learning但是啊但是machine learning它有一个问题就是它只会英武学舍它不能理解当然这两个词都非常的重所以说我们接下来就要去讲这个什么是理解在这里边有一个window grade发现车是可以在红绿灯前面停下来的车可以压碎它的坚果然后它就会把坚果drop到这个红绿灯前面让车去把它压碎然后等到红灯的时候再去把这个坚果给pick up起来在这里边它只有一次任务就是它如果被车撞了就被撞死了所以说它所有过去的MOS', metadata={'source': '7'}),\n",
       " Document(page_content='Auto-Regressive Large Language Model它是一个代言模型然后这个代言模型它的本质是一个生成式的然后它是一个Auto-Regressive就是自回归式的那这个具体怎么理解就是它是这个是一个就是Stefan Wolfram的一个图生成下一个次然后你这样自然而然就可以把人类已知的文本和它去进行一个匹配看它生成的对不对这样的话你就有海量的标签好的数据去帮助你这个模型去学习之后就是这个他们非常重要的scaling law的这个observation我在这儿稍微再多解释就在这里就多解释一下吧为什么generative这个任务如此之重要标注都是所谓的labeled包括你的这个就是unsupervised learning其实你在一定程度上也必须要告诉这个模型的做得好还是不好那这样的数据其实是有限的和人需要去生成的也就是说你不能让模型去举一反三你只能让模型去做一个已经限定好的任务下去进行优化可是generative呢这个事情就变得非常的可以多学到这里面的知识和多给模型为高质量的文本接下来就是一个Leap of', metadata={'source': '2'}),\n",
       " Document(page_content='of Faith就是你会相信这么一个大模型能产出好的结果这个Leap of Faith非常的重要但是OpenAI的人很明显是有这样的一个信仰的他们也做到了这些技术细节我们就不多聊了就是接下来一个技术很重要的点就是In-Context Learning然后这一点其实从一定程度上也是可以这么说的因为fantuning这个词并不是特别的精确所以呢我们这儿就发现fantuning其实是一个过度browt 过度宽泛的一个词那我们怎么样子去区别呢我们就区别过去的Machinery Model 是需要change the weight of the base model但是 in context learning 呢它其实是 activatethe different ways in the base model它这个是很重要的这就是区别了就traditional如果说一个high level的解释的话traditional ML是new task需要new modelin context learning呢可以让你做到new tasksame modeldifferent', metadata={'source': '3'})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity search\n",
    "# docs = speech_vector.max_marginal_relevance_search(query, k=5, fetch_k=28, lambda_mult=0.5)\n",
    "docs = speech_vector.similarity_search(query, )\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a9a0970-159a-4ce8-9914-cf8f14887260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提升大模型推理能力的关键在于几个技术点：\n",
      "\n",
      "1. In-Context Learning：这种学习方式允许模型在上下文中激活不同基础模型的方式，从而提高了处理新任务的能力。\n",
      "\n",
      "2. Generative Task Importance：生成式任务对于训练高质量的文本和提升模型性能至关重要。通过生成样本并评估其质量，模型可以持续优化。\n",
      "\n",
      "3. Scaling Law Observation and Leap of Faith：研究者观察到的关于大模型的规模定律以及对模型能产出高质量结果的信心（Leap of Faith），也是推动模型推理能力提升的重要因素。\n",
      "\n",
      "综上所述，在提升大模型推理能力的过程中，技术手段、任务重要性认识以及信心的树立等方面都发挥着关键作用。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using chain for the query\n",
    "response = chain.invoke(\n",
    "    input={'input_documents': docs, 'question': query}, \n",
    "    # return_only_outputs=True,\n",
    ")\n",
    "\n",
    "print(response[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00184d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44     之后就是这个他们非常重要的scaling law的这个observation\n",
       "144            就是GUI加Morse law for everything\n",
       "199                                 它是ScanIt吗\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmppryf5wq2.wav':\n",
      "  Duration: 00:00:04.94, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   4.84 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpygjnur2c.wav':\n",
      "  Duration: 00:00:02.00, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.94 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpyzllyq5z.wav':\n",
      "  Duration: 00:00:02.00, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.92 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "playback_by_query(\"Scaling Law\", 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b62cdf7d-f116-48a6-af79-09c4b1a4efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提升大模型推理能力的关键在于几个方面：\n",
      "\n",
      "1. **生成式学习**：通过训练模型生成文本，这种自回归式的任务有助于提高理解力。\n",
      "\n",
      "2. **上下文学习（In-Context Learning）**：通过在给定的上下文中展示特定的任务或例子，模型可以在无需额外训练的情况下学会新的任务和模式。\n",
      "\n",
      "3. **强化学习与反馈**：通过与用户交互，并根据用户的反馈调整其输出，这有助于提升模型在理解人类意图方面的表现。\n",
      "\n",
      "这些方法和技术相互结合，共同推动大模型推理能力的提升。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'input_documents': docs, 'question': query},)['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87be17ef-d582-43a4-bdb8-5fbcb17f5680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73         然后reinforcement learning with human feedback\n",
       "81    reinforcement learning with human feedback其实如果...\n",
       "47    为什么generative这个任务如此之重要标注都是所谓的labeled包括你的这个就是un...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpj2372qb4.wav':\n",
      "  Duration: 00:00:02.80, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   2.70 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpc5m7s1lm.wav':\n",
      "  Duration: 00:00:31.86, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "  31.79 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpp05k_dw3.wav':\n",
      "  Duration: 00:00:34.42, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "  34.35 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "playback_by_query(\"强化学习与反馈\", 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49748615-284c-42bd-9592-7fe571fdaf96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97d17182-325b-462c-8ade-cc007780f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# setup llm\n",
    "# local_llm = 'wizardlm2:7b-fp16'\n",
    "# local_llm = 'llama3:8b-instruct-fp16'\n",
    "llm = ChatOllama(model=local_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbb39d3b-30f3-4187-a610-39238a88eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retriever --> equvalent to vector search\n",
    "retriever = speech_vector.as_retriever(\n",
    "    search_type='similarity',  # similarity, mmr, similarity_score_threshold\n",
    "    search_kwargs={'k':4, },  # k, score_threshold\n",
    ")\n",
    "\n",
    "# check retriever\n",
    "docs = retriever.invoke(query)\n",
    "assert docs == speech_vector.similarity_search(query, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef505d09-2a7a-43e4-94f4-76277e4950f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e9ea4c4-963f-4a8a-ad13-38e87a9dab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提升大模型的推理能力可以通过多种方式进行。首先，涌现（emergence）是一个关键点，它指的是模型在某个节点上突然改善或学习新能力的过程。这并不是一个线性的变化，而是模型在训练过程中达到一个阶段性的转折点后发生的。GPT模型通常使用无监督学习（unsupervised learning）作为基础模型，然后通过监督细化（supervised fine-tuning）来进行对齐（alignment）。这个对齐过程中，可以引入强化学习（reinforcement learning）与人类反馈来改善模型的输出，使其更符合人类的理解和期望。\n",
      "\n",
      "其次，理解（understanding）是机器学习中一个重要的概念。模型能否找到数据中的规律并理解这些规律的关键在于。例如，模型可以学习通过观察数据来识别和执行任务，如识别车辆是否可以在红绿灯前停留等。\n",
      "\n",
      "第三，作为一个自回归式的生成模型，GPT可以根据先前生成的内容生成后续内容，这种自然语言处理能力使得模型能够匹配和生成高质量的文本。此外，模型的学习效率受到所谓“缩放定律”（scaling law）的限制，这意味着模型的大小与数据量的关系在一定程度上决定了其性能。\n",
      "\n",
      "最后，信仰（Leap of Faith）是实现高质量结果的关键。OpenAI团队对此有着强烈的信念，并通过技术细节来实现这一点。在这个过程中，新兴的技术如内上下文学习（In-Context Learning）也显得尤为重要，它允许模型在不改变基础模型的情况下针对新任务进行适应和优化。这种方法与传统的机器学习模式有所不同，后者通常需要为每个新任务创建一个新的模型。\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "038c2548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如何提升大模型的推理能力？\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79                                             去理解模型在想什么\n",
       "111                                        它是在数据里面寻找一些规律\n",
       "23     而不是一个范式突破的话那它我们如何使用大圆模型第五个是我们人类和大圆模型有什么不一样这四个问...\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpfhmplgcf.wav':\n",
      "  Duration: 00:00:01.60, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.50 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpxc1p6tlr.wav':\n",
      "  Duration: 00:00:03.06, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   2.93 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmplno_ggbg.wav':\n",
      "  Duration: 00:00:31.89, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "  31.80 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "playback_by_query(\"模型能否找到数据中的规律并理解这些规律的关键在于\", 3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae77a89-da81-42ef-bcff-dd4e29710244",
   "metadata": {},
   "source": [
    "#### 2.4 Meeting Minutes Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cc46f412-f6a2-4c6b-a7c1-fd8f1f0fab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(transcribe_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe788204-07f1-43b1-bcfb-9198eacdd95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary prompt\n",
    "summary_prompt_template = \"\"\"Your goal is to summarize the meeting transcription that is given to you as the following:\n",
    "                \"{text}\"\n",
    "                The summarization of the meeting minutes shall limit to 2500 words.\n",
    "                Only output the summary without any additional text.\n",
    "                Focus on providing a summary in a structured format text of what subject reviewed and the action items out of it.\n",
    "                \"\"\"\n",
    "\n",
    "prompt_test = \"\"\"\n",
    "You are a commentator. Your task is to write a report on a meeting transcription. \n",
    "When presented with the meeting minutes, come up with interesting questions to ask,\n",
    "and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown\n",
    "format. \n",
    "Focus on providing a summary in a structured format text of the overall performance rating and the action items out of it.\n",
    "\n",
    "# Meeting Keynotes: \n",
    "\"{text}\"\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, use only the context information, to summarize the key points of:\n",
    "- Overall Performance ratings by any of the [Green, Amber, Red]\n",
    "- Financial status\n",
    "- Projects status\n",
    "- Action items\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be \n",
    "asked about the content of the meeting. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed \n",
    "in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a analysis report\n",
    "Using the summary and the answers to the interesting questions, \n",
    "create a comprehensive report in Markdown format. \n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(prompt_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55774c7a-1cbc-45d0-893a-99d83894ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary:\n",
      "The speaker discusses the evolution of technology, particularly focusing on the advancements in AI and machine learning, exemplified by GPT (Generative Pre-trained Transformer) models like ChatGPT. They highlight the shift from specific computer solutions to more general AI that can understand and interact with users naturally. The speaker emphasizes the importance of using GPT not just as a tool but also as a partner that can autonomously perform tasks such as data analysis, given proper training and alignment.\n",
      "\n",
      "They suggest that in the past year, the focus should have been on teaching GPT how to perform tasks (engineering ability) and moving forward, it will be more about telling GPT what to do (alignment). The speaker also touches upon historical examples like XGBT and Google, illustrating how new technology can create vast opportunities by doing what previous technologies couldn't. They mention Instagram as an example of a simple tool that became a social media platform because it met a genuine user need.\n",
      "\n",
      "The speaker posits that GPT represents a significant 2B2C (business-to-business and business-to-consumer) opportunity, though not necessarily in its current form as \"ScanIt\" or \"Private Search\" functionalities. They conclude by distinguishing between the capabilities of GPT and human Eureka moments, where humans excel at finding and prioritizing truly impactful answers among many options.\n",
      "\n",
      "### Interesting Questions:\n",
      "1. **Q:** How does the evolution of AI models like ChatGPT impact the future job market, and what skills will be essential for individuals to remain relevant?\n",
      "   **A:** The evolution of AI models like ChatGPT is automating a wide range of tasks that were previously performed by humans. This shift is likely to continue, impacting various job sectors. To remain relevant, individuals should focus on developing skills that complement AI, such as critical thinking, creativity, emotional intelligence, and complex problem-solving abilities. Additionally, understanding how to work alongside AI and leveraging its capabilities effectively will be crucial for future employment opportunities.\n",
      "2. **Q:** Given the advancements in AI like ChatGPT, what are the potential ethical issues that may arise?\n",
      "   **A:** As AI models like ChatGPT become more autonomous, several ethical issues may arise. These include concerns about privacy, the potential for AI to perpetuate biases present in data, the impact of AI on the labor market, and the challenges associated with holding AI accountable for its actions. There is also the question of how AI should be regulated to prevent misuse or harm. Moreover, there are issues related to the potential for AI to replace human jobs, which could lead to significant social and economic changes.\n",
      "3. **Q:** What are the key differences between the capabilities of GPT models and human cognitive abilities?\n",
      "   **A:** While GPT models like ChatGPT have made significant strides in natural language understanding and generation, they still fall short of human cognitive abilities. Humans possess a unique set of skills that includes but is not limited to abstract reasoning, creativity, empathy, and the ability to have 'Eureka' moments where they can connect disparate ideas and insights in novel ways. Unlike GPT models, humans can intuitively grasp complex concepts, draw connections between seemingly unrelated phenomena, and creatively synthesize new ideas or solutions from existing knowledge. Additionally, humans are capable of moral reasoning, emotional responses, and have the innate ability to question their own logic and assumptions, which is something that current AI models lack.\n",
      "### Analysis Report:\n",
      "# Meeting Analysis Report\n",
      "## Overview\n",
      "In this meeting, we discussed the transformative nature of AI technology, particularly focusing on the advancements in AI models like ChatGPT. These models represent a significant shift from specific computer solutions to more general AI that can understand and interact with users naturally. The key points of our discussion include:\n",
      "- The importance of using GPT not just as a tool but also as a partner that can autonomously perform tasks such as data analysis, given proper training and alignment.\n",
      "- The historical context provided by examples like XGBT and Google, illustrating how new technology creates vast opportunities by doing what previous technologies couldn't.\n",
      "- The shift from focusing on teaching GPT how to perform tasks (engineering ability) to moving forward with telling GPT what to do (alignment).\n",
      "- The distinction between the capabilities of GPT and human Eureka moments, where humans excel at finding and prioritizing truly impactful answers among many options.\n",
      "## Financial & Projects Status\n",
      "The financial status was not explicitly detailed in the provided context. However, it was implied that the evolution of AI models like ChatGPT could have significant implications for the job market and the skills individuals will need to remain relevant. Additionally, the status of various projects was not explicitly mentioned but was implicitly tied to the development of these AI models.\n",
      "## Action Items\n",
      "Based on the discussion, the following action items were identified:\n",
      "- Focus on teaching GPT how to perform tasks (engineering ability).\n",
      "- Prepare for moving forward with telling GPT what to do (alignment).\n",
      "- Develop a comprehensive understanding of how to work alongside AI like ChatGPT.\n",
      "## Thought-Provoking Questions and Answers\n",
      "1. **Q:** How does the evolution of AI models like ChatGPT impact the future job market, and what skills will be essential for individuals to remain relevant?\n",
      "   **A:** The evolution of AI models like ChatGPT is automating a wide range of tasks that were previously performed by humans. This shift is likely to continue, impacting various job sectors. To remain relevant, individuals should focus on developing skills that complement AI, such as critical thinking, creativity, emotional intelligence, and complex problem-solving abilities. Additionally, understanding how to work alongside AI and leveraging its capabilities effectively will be crucial for future job opportunities.\n",
      "2. **Q:** What are the potential ethical issues that may arise with the advancements in AI AI models like ChatGPT?\n",
      "   **A:** As AI models like ChatGPT become more autonomous, several potential ethical issues may arise, including concerns about privacy, the potential for AI to perpetuate biases present in data, the impact of AI on the labor market, and the challenges associated with holding AI accountable for its actions. There is also the question of how AI should be regulated to prevent misuse or harm. Moreover, there are issues related to the potential for AI to replace human jobs, which could lead to significant social and economic changes.\n",
      "3. **Q:** What are the key differences between the capabilities of GPT models like ChatGPT and human cognitive abilities?\n",
      "   **A:** While GPT models like ChatGPT have made significant strides in natural language understanding and generation, they still fall short of human cognitive abilities. Humans possess a unique set of skills that includes but is not limited to abstract reasoning, creativity, empathy, and the ability to have 'Eureka' moments where they can connect disparate ideas and insights in novel ways. (continued)\n",
      "```\n",
      "## Additional Notes\n",
      "- The discussion highlighted the transformative nature of AI technology, with particular focus on the advancements in AI models like ChatGPT.\n",
      "- It was emphasized that GPT models represent a significant shift from specific computer solutions to more general AI that can understand and interact with users naturally.\n",
      "- Historical context provided by examples like XGBT and Google was used to illustrate how new technology creates vast opportunities by doing what previous technologies couldn't.\n",
      "- The shift from focusing on teaching GPT how to perform tasks (engineering ability) to moving forward with telling GPT what to do (alignment) was a key point of the discussion.\n",
      "- The distinction between the capabilities of GPT and human Eureka moments was made, with humans excelling at finding and prioritizing truly impactful answers among many options.\n",
      "## Conclusion\n",
      "The conclusion of this meeting's analysis is that AI technology, particularly the advancements in AI models like ChatGPT, holds significant implications for the future job market and the skills individuals will need to remain relevant. The ability of these AI models to autonomously perform tasks represents a transformative shift from specific computer solutions to more general AI capable of understanding and interacting with users naturally. As we continue to develop these AI models further, it will be crucial to focus on teaching them how to perform tasks and prepare for moving forward with telling them what to do. Additionally, understanding how to work alongside AI like ChatGPT will be an increasingly important skill set for individuals looking to remain relevant in the evolving landscape of AI technology advancements.\n",
      "---\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "docs = [Document(page_content=transcribe_text, metadata={\"source\": \"local\"})]\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
    "\n",
    "print(stuff_chain.invoke(docs)['output_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0555af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192                                           就比如说Google\n",
       "193    Google在一开始的互联网上是没有用的因为那个时候互联网上都没有什么信息就是start w...\n",
       "170                                             而是应该去做网页\n",
       "Name: text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpybri4uua.wav':\n",
      "  Duration: 00:00:01.44, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.31 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmph84e8a5g.wav':\n",
      "  Duration: 00:00:39.31, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "  39.20 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/lv/4kql5s856s56ycnzm1ly8y0m0000gn/T/tmpi6yxwg1w.wav':\n",
      "  Duration: 00:00:01.74, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, 2 channels, s16, 1536 kb/s\n",
      "   1.66 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "playback_by_query(\"Instagram and Google\", 3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e716d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
